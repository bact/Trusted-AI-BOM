# Identification and Attestation

The purpose of TAIBOM is to allow anyone to make an attestation about any AI system or component, where an attestation consists of the identity of the attestor and the identity of the AI system or component that the attestation is about. Central to the success of this is the ability to determine which attestations apply to which AI systems or components in a consistent and repeatable way, but this determination might be subjective. For example, one organization might consider two AI systems to be different depending on where they are hosted even if they are otherwise identical, whereas another might consider them to be different. TAIBOM therefore seeks to provide the means to represent the information that users might need to determine what matters in their own use case rather than being prescriptive.

Of central importance, however, are likely to be factors that determine an AI system or component's inference behaviour. The inference behaviour of an AI system is important because almost everything except the system's legal status relates to its inference behaviour: its performance; its security status; its reliability; etc. and hence if two systems have different inference behaviour, they will almost certainly need to be considered to be different systems. Depending on the type of the AI system, it's behaviour is potentially influenced by a wide range of factors, such as it's training data; the training algorithm; the code used for training; the inference algorithm; the code used for inference; the weights used for inference; its training and inference configurations; adaptors used during inference; data used during inference; etc.

Some of these factors have a hierarchical structure. For example, in terms of inference behaviour, the code used for inference will cluster beneath the inference algorithm, where different implementations of the same algorithm might be provided within different frameworks or on different platforms. In some cases, attestations might relate to the use of a particular algorithm irrespective of the framework within which it is implemented, but, in other cases, the attestations might only relate to a specific implementation of a specific algorithm within a specific framework.

Similarly, we might expect to see the behaviour of the system conditionally independent of some factors in the presence of others. For example, the combination of weights and code used for inference might be sufficient to completely determine the behaviour of the system during inference, in which case, in terms of behavioural attestations, the training data, code and algorithm become irrelevant in the sense that two systems with identical weights and inference code will exhibit the same behaviours regardless of differences in training algorithm and hence all behavioural attestations that apply to one will also apply to the other.

The behaviour of an AI system might also be affected by data that are retrieved during inference. For example, a system might use a private corpus to answer questions from a user. How such a system is represented in TAIBOM depends on where the boundary of the system is drawn: if the boundary is drawn around the AI system that does the inference and all the data that it can access, then the system can be assigned attestations that are static in the sense that they are a function of the system only and do not vary for each inference.

In practice, however, such a boundary might be so large as to be almost useless. Consider, for example, a system that uses the internet to answer questions. Including the entire internet within the boundary of the system is likely to allow for only very limited attestations in many cases. It is therefore also likely to be useful to draw a boundary around the AI system that does the inference and the specific data that it accessed during a specific inference. This creates a dynamic TAIBOM that reflects the fact that the behaviour of the system might change with each inference.

This could be achieved by associating TAIBOMs with individual webpages that are known to contain content that adversely affects certain types of AI systems, which is conceptually similar to maintaining a reputation list. For example, webpages have been created that attempt to modify the behaviour of Bing Chat. By providing a TAIBOM for such pages, it would be possible to provide a dynamic TAIBOM for Bing Chat that reflects that fact that its properties vary dynamically on a per inference basis. Similarly, we are starting to see the emergence of models that dynamically apply different mixes of adaptors or invoke different mixtures of experts or external software tools at inference time. Again, static attestations of such systems are able to account for the gross properties of the system but fail to represent the fact that their properties will vary for each inference.

An additional consideration is that some AI systems are stateful in the sense that their behaviours are determined by their histories. Consider, for example, a chatbot that can access the internet and can also automatically create and maintain a list of useful facts and use them during a conversation. The behaviour of the chatbot will be a function of the deep history of the conversation and not just the history that fits in its context window, the user's current utterance or the currently retrieved webpages. As before, such a system could be described by broad static attestations or by more specific dynamic attestations that change for each inference. The key difference from the previous example, however, is that the dynamic memory makes it possible that accessing a bad webpage sometime ago could affect the system's trustworthiness much later.

A final consideration relating to inference behaviour is that the input to an AI system or component - the data for which inference is requested - might affect its behaviour. For example, some attestations might apply only when the input is, in some sense, "in distribution". Again, this implies a that TAIBOM should be able to support dynamic attestations that vary on each inference.

Subjectivity also applies not only in system or component identification, but also as to whether an organization trusts an attestor. For example, one organization might maintain a list of trusted third parties and trust only their attestations, whereas another might also trust attestations provided by the creators of an AI system. TAIBOM should therefore not be prescriptive about determining which attestations to trust, but should provide the means to represent the information that users might need to make their own determination as to whether they trust the source of an attestation. This includes not just the source itself, but also its context and the context of the attestation. For example, an organization might choose not to trust attestations originating from companies headquartered in China or they might choose to trust such attestations only when they relate to components developed by companies that are not also headquartered in China.

TAIBOM also needs to be flexible in the content of attestations, which might need to represent information such as: system performance metrics, such as speed of response, throughput, latency, uptime, as well as accuracy and precision; intended, prohibited or out-of-scope use cases; etc. and hence should not be unnecessarily prescriptive. It is important, however, that there is a naming convention for the content of attestations so that it is possible to query a database of attestations for particular content and to detect conflicting attestations.

The legal status of an AI system or component is likely to be particularly important as dataset, model and open weight licences often restrict use, such as prohibiting use in military applications or prohibiting commercial use; they might include information about privacy as well as data processing and collection policies; and might offer indemnifications, such as against claims for copyright infringement or other forms of damage or loss. It is possible that the legal status of an AI system or component will vary depending on how it is obtained or based on other commercial arrangements that might not be determinable from a technical analysis of the AI system or component itself.

Factors affecting the legal status of an AI system or component might include: training and inference software licences and terms of use; data licences and terms of use; laws and regulations relating to the location in which training or inference occurs, or where the inferences are used; laws and regulations in the country where the provider of the AI system or component is headquartered; licences and terms of use of the hosting provider; licences and terms of use of a service provider; additional commercial agreements; etc.

The TAIBOM of an AI system is compositional and will inherit the properties of its components. For example, if a system is composed of two third party AI components, one of which was trained on a dataset containing data with a license that prohibited military use, then the system as a whole is likely to be subject to the same prohibition. This compositionality also applies between components such as the hardware, firmware and software of the training and inference environments and any associated licences and legal agreements. 

Rules of composition. For example, software vulnerabilities relating to the training software are not inherited by the inference software

#  Labelling/Versioning

Every aspect of a complex AI system needs labelling and versioning. (data, code and physical systems). Ideally there should be a method of attesting to the version. There can be various trust models to implement this

# Dependencies

A complex AI system has dependencies that need describing to fully understand provenance. TAIBOM will provide an interoperable method of describing these dependencies 

#  Attestation

Any actor (author or third party) can provide descriptors for each component of the system as a whole. (e.g. a training content review, as SBOM validation, a system integrity check, a fairness assessment). 

TAIBOM provides both a mechanism of making these attestations, but also a framework for the dynamic and subjective evaluation, of combinations of these at