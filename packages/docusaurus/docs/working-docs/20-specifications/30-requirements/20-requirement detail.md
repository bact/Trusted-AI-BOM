# Requirements Overview

The core elements of TAIBOM are identification and attestation. In this section, we consider each of these in more detail and identify a wide range of requirements. We finish by describing a specific subset of those requirements that are particularly useful and that we intend to focus on during the course of this project.


## System and Component Identification

TAIBOM attestations communicate claims about the properties of an identifiable entity or set of entities. As was previously discussed, TAIBOM needs to be flexible with regards to what is identified and the means by which it is identified. This is because different parties will be interested in making, or searching for, claims about the properties of different sets of entities.

For example, a TAIBOM claim might assert that:

* a particular dataset is poisoned in a particular way
* a particular dataset is composed from other particular datasets
* particular inference code on a particular framework on a particular platform has a vulnerability
* particular inference code and weights applied to a particular dataset achieve a particular level of performance on a particular benchmark
* a particular benchmark tests memorisation in a particular domain
* a particular inference was generated by a particular combination of inference code, weights and mix of adaptors
* a particular set of weights was generated using particular training code, a particular training configuration and a particular mix of datasets
* particular training code implements a particular training algorithm
* a particular webpage contains a prompt injection attack
* a set of weights was downloaded from a particular source
* a set of weights were produced by a particular company
* a set of weights are associated with particular terms of use
* an inference was obtained from a particular service provider
* a particular service provider provides a particular indemnification

Each claim should, ideally, identify the largest set of entities to which it applies. If, for example, a particular dataset contains poisoned data, we can reason broadly about the impact of the poisoning if we have a claim relating to the poisoning of the dataset, claims informing us what other datasets incorporate the dataset and claims telling us which model weights were produced by training on which datasets. If we only have a vey narrow claim that a particular set of weights were trained on poisoned data, we are unable to generalise.

Similarly, when users search for TAIBOMs, they should be able to specify complex queries - that is, they should be able to specify sets of entities of interest in complex ways. For example, someone might want to find weights trained on data excluding a particular dataset, that achieves a specified minimum level of performance on a benchmark, as attested to by a party other than the one that trained the model; or they might want to find the best performing such model that has terms of use that permit commercial use; etc.

Many of TAIBOM's use cases relate either to the inference behaviour of a system or its legal status and it is therefore essential that TAIBOM is able to distinguish between systems with different inference behaviours and legal statuses.

## Factors Affecting Inference Behaviour

Depending on the type of the AI system, it's inference behaviour is potentially influenced by a wide range of factors, such as it's training data; the training algorithm; the code used for training; the inference algorithm; the code used for inference; the weights used for inference; its training and inference configurations; adaptors used during inference; data used during inference; etc.

Some of these factors have a hierarchical structure. For example, in terms of inference behaviour, the code used for inference is likely to form a behavioural cluster beneath the inference algorithm, where different implementations of the same algorithm might be provided within different frameworks or on different platforms, with small variations in behaviour. In some cases, attestations might relate to the use of a particular algorithm irrespective of the detail of how it is implemented, but, in other cases, attestations might relate to a specific implementation of a specific algorithm within a specific framework.

Similarly, we might expect to see the inference behaviour of a system conditionally independent of some factors in the presence of others. For example, the combination of weights and code used for inference might be sufficient to completely determine the behaviour of the system during inference, in which case, in terms of behavioural attestations, the training data, training code and training algorithm become irrelevant in the sense that two systems with identical weights and inference code will exhibit the same inference behaviour regardless of differences in how they were trained and hence all attestations that relate to inference behaviour that apply to one will also apply to the other.

It should be noted, however, that having information about how a model is trained might still be useful. It is possible, for example, that systems trained using a particular approach tend to have specific properties, such as adversarial robustness. Providing information about how a model was trained will, in some cases, therefore make it possible to infer certain properties of its inference behaviour even when there are no attestations that are explicitly about the inference behaviour.

The inference behaviour of an AI system might also be affected by data that are retrieved during inference. For example, a system might use a private corpus to answer questions from a user. How such a system is represented in TAIBOM depends on where the boundary of the system is drawn: if the boundary is drawn around the AI system that does the inference and all the data that it has access to, then the system can be assigned attestations that are static in the sense that they are the same for each inference.

In practice, however, such a boundary might be so large as to be almost useless. Consider, for example, a system that uses the internet to answer questions. Including the entire internet within the boundary of the system is likely to allow for only very limited attestations in many cases, particularly when it comes to properties like the system's security and the factuality of its outputs.

In such cases, it might be useful to draw a more specific boundary around the AI system that includes only the specific data that were accessed during a specific inference. This creates a dynamic inference-level TAIBOM that supplements the static system-level TAIBOM and reflects the fact that the effective structure and content of the system changes with each inference.

TAIBOMs could be associated with webpages that are known to contain content that adversely affects certain types of AI systems, which is conceptually similar to maintaining a reputation list or blacklist. For example, webpages have been created that attempt to modify the behaviour of Bing Chat. By providing a TAIBOM for such pages, it would be possible to provide a dynamic supplemental TAIBOM for Bing Chat that reflects the fact that some of its properties, such as its trustworthiness and the factual accuracy of its outputs, vary on a per inference basis. 

Similarly, we are starting to see the emergence of models that dynamically apply different mixtures of adaptors or invoke different mixtures of experts or external tools at inference time. Again, static TAIBOM attestations for such systems can account for their static or gross properties, while dynamic supplemental TAIBOMs can be issued per inference to account for the dynamic structure of the system and its inference specific properties.

An additional consideration is that some AI systems are stateful in the sense that their behaviours are determined by their histories. Consider, for example, a chatbot that can access the internet and also automatically creates and maintains a list of useful facts and uses them during a conversation. The behaviour of the chatbot will be a function of the deep history of the conversation and not just its recent history or the currently retrieved webpage. 

Such a system could be described by the static and dynamic TAIBOMs that have already been discussed, but with the difference that the properties attested to in the per inference TAIBOM might be affected by the entire history of the system rather than just the current inference.

A final consideration relating to inference behaviour is that the primary input to an AI system (in contrast to retrieved content) might also affects its behaviour in a way that might have implications for inference behaviour-related attestations. For example, some attestations might apply only when the input is, in some sense, "in distribution" or consistent with the system's intended use. Again, this implies a that TAIBOM should be able to support dynamic attestations that vary on each inference.

This section has discussed factors affecting the inference behaviour of an AI system. The following section discusses the legal statuses of AI systems.


## Factors Affecting a System's Legal Status

The legal status of an AI system is intended to cover anything relating to the law, including licences, terms of use and regulations, as well a legal risks that might arise from the use of AI models, such as those relating to copyright. The legal status of a model might restrict its use, typically preventing commercial or military use, but might also offer indemnifications against loses arising from claims of copyright infringement.

The legal status of a system is typically affected by the datasets on which its components were trained, the creators of components, the suppliers of components, service providers, hosts, and the laws and regulations local to where the various components were developed, are hosted, are used and, potentially, where data originate. TAIBOM needs to be able to express the legal status of a system, making good use of compositionality to maximize its ability to generalise attestations.

For example, rather than a particular system hosted with a particular provider having the provider's terms associated with it individually, it is better to have the provider as a property of the system and the provider's terms be a property of the provider. This allows the terms for all systems with the same host to be updated automatically when the host's terms are updated and avoids the need to iterate of all system TAIBOMs and update them individually.

Composing legal constraints is typically easier than composing other types of properties because it is generally the case that the constraints on the system are the tightest of the constraints of its components. For example, if all components in a system have permissive licences, but one prohibits commercial use, then commercial use the system as a whole will also be prohibited.

The legal constrains on a system might also be relatively arbitrary in the sense that a particular buyer might obtain specific legal terms that do not apply to identical copies of the same system that are used by others. TAIBOM therefore needs to be able to represent such special cases and avoid overgeneralization.


## Other factors

We have so far focused on the inference behaviour and legal status of a system as they account for a large proportion of what stakeholders of TAIBOM are likely to care about. When it comes to trust more generally, however, there are other factors that might be relevant, such as the country where a system is hosted.


## Combining Attestations

Combining entities is common in the AI space. For example, datasets are frequently remixed into different combinations, or built upon; and model weights are combined with adaptors; etc. TAIBOM needs to be able to approximate the effects of forming such combinations so that we can efficiently infer the likely properties of the combined entity from its components. We use the word "approximate" here because the effects of combination may be complex and depend on what are being combined.

For example, combining a base model with an adaptor does not necessarily preserve the properties of either the base model or the adaptor. Similarly, combining datasets is compositional in terms of some properties, such as whether the combined dataset is poisoned, but not necessarily in terms of others, such as whether the combined dataset is statistically biased in some particular way. We therefore need to be clear as to which combinations produce composition and which produce modulation with respect to which properties. 

In other words:

* Properties of components are modulatory with respect to other properties of the combined entity if the latter depend on a specific combination of the former and hence it is not possible to generalise the latter from one combination to another.

* Properties of components are compositional if the combined entity directly inherits the properties of its components.

So, for example, applying an adaptor or combining training datasets is modulatory with respect to model inference behaviour and hence any property that is a function of inference behaviour cannot be generalized across adaptors or differently composed training datasets. On the other hand, whether a training dataset contains poisoned data, contains copyrighted material, or has terms of use that prohibit the commercial use of trained models are compositional and hence can be generalized across datasets based on their components.

TAIBOM should recognize the distinction between modulatory and compositional properties.









==========================================================


# Identification and Attestation

The purpose of TAIBOM is to allow anyone to make an attestation about any AI system or component, where an attestation consists of the identity of the attestor and the identity of the AI system or component that the attestation is about. Central to the success of this is the ability to determine which attestations apply to which AI systems or components in a consistent and repeatable way, but this determination might be subjective. For example, one organization might consider two AI systems to be different depending on where they are hosted even if they are otherwise identical, whereas another might consider them to be different. TAIBOM therefore seeks to provide the means to represent the information that users might need to determine what matters in their own use case rather than being prescriptive.

Of central importance, however, are likely to be factors that determine an AI system or component's inference behaviour. The inference behaviour of an AI system is important because almost everything except the system's legal status relates to its inference behaviour: its performance; its security status; its reliability; etc. and hence if two systems have different inference behaviour, they will almost certainly need to be considered to be different systems. Depending on the type of the AI system, it's behaviour is potentially influenced by a wide range of factors, such as it's training data; the training algorithm; the code used for training; the inference algorithm; the code used for inference; the weights used for inference; its training and inference configurations; adaptors used during inference; data used during inference; etc.

Some of these factors have a hierarchical structure. For example, in terms of inference behaviour, the code used for inference will cluster beneath the inference algorithm, where different implementations of the same algorithm might be provided within different frameworks or on different platforms. In some cases, attestations might relate to the use of a particular algorithm irrespective of the framework within which it is implemented, but, in other cases, the attestations might only relate to a specific implementation of a specific algorithm within a specific framework.

Similarly, we might expect to see the behaviour of the system conditionally independent of some factors in the presence of others. For example, the combination of weights and code used for inference might be sufficient to completely determine the behaviour of the system during inference, in which case, in terms of behavioural attestations, the training data, code and algorithm become irrelevant in the sense that two systems with identical weights and inference code will exhibit the same behaviours regardless of differences in training algorithm and hence all behavioural attestations that apply to one will also apply to the other.

The behaviour of an AI system might also be affected by data that are retrieved during inference. For example, a system might use a private corpus to answer questions from a user. How such a system is represented in TAIBOM depends on where the boundary of the system is drawn: if the boundary is drawn around the AI system that does the inference and all the data that it can access, then the system can be assigned attestations that are static in the sense that they are a function of the system only and do not vary for each inference.

In practice, however, such a boundary might be so large as to be almost useless. Consider, for example, a system that uses the internet to answer questions. Including the entire internet within the boundary of the system is likely to allow for only very limited attestations in many cases. It is therefore also likely to be useful to draw a boundary around the AI system that does the inference and the specific data that it accessed during a specific inference. This creates a dynamic TAIBOM that reflects the fact that the behaviour of the system might change with each inference.

This could be achieved by associating TAIBOMs with individual webpages that are known to contain content that adversely affects certain types of AI systems, which is conceptually similar to maintaining a reputation list. For example, webpages have been created that attempt to modify the behaviour of Bing Chat. By providing a TAIBOM for such pages, it would be possible to provide a dynamic TAIBOM for Bing Chat that reflects that fact that its properties vary dynamically on a per inference basis. Similarly, we are starting to see the emergence of models that dynamically apply different mixes of adaptors or invoke different mixtures of experts or external software tools at inference time. Again, static attestations of such systems are able to account for the gross properties of the system but fail to represent the fact that their properties will vary for each inference.

An additional consideration is that some AI systems are stateful in the sense that their behaviours are determined by their histories. Consider, for example, a chatbot that can access the internet and can also automatically create and maintain a list of useful facts and use them during a conversation. The behaviour of the chatbot will be a function of the deep history of the conversation and not just the history that fits in its context window, the user's current utterance or the currently retrieved webpages. As before, such a system could be described by broad static attestations or by more specific dynamic attestations that change for each inference. The key difference from the previous example, however, is that the dynamic memory makes it possible that accessing a bad webpage sometime ago could affect the system's trustworthiness much later.

A final consideration relating to inference behaviour is that the input to an AI system or component - the data for which inference is requested - might affect its behaviour. For example, some attestations might apply only when the input is, in some sense, "in distribution". Again, this implies a that TAIBOM should be able to support dynamic attestations that vary on each inference.

Subjectivity also applies not only in system or component identification, but also as to whether an organization trusts an attestor. For example, one organization might maintain a list of trusted third parties and trust only their attestations, whereas another might also trust attestations provided by the creators of an AI system. TAIBOM should therefore not be prescriptive about determining which attestations to trust, but should provide the means to represent the information that users might need to make their own determination as to whether they trust the source of an attestation. This includes not just the source itself, but also its context and the context of the attestation. For example, an organization might choose not to trust attestations originating from companies headquartered in China or they might choose to trust such attestations only when they relate to components developed by companies that are not also headquartered in China.

TAIBOM also needs to be flexible in the content of attestations, which might need to represent information such as: system performance metrics, such as speed of response, throughput, latency, uptime, as well as accuracy and precision; intended, prohibited or out-of-scope use cases; etc. and hence should not be unnecessarily prescriptive. It is important, however, that there is a naming convention for the content of attestations so that it is possible to query a database of attestations for particular content and to detect conflicting attestations.

The legal status of an AI system or component is likely to be particularly important as dataset, model and open weight licences often restrict use, such as prohibiting use in military applications or prohibiting commercial use; they might include information about privacy as well as data processing and collection policies; and might offer indemnifications, such as against claims for copyright infringement or other forms of damage or loss. It is possible that the legal status of an AI system or component will vary depending on how it is obtained or based on other commercial arrangements that might not be determinable from a technical analysis of the AI system or component itself.

Factors affecting the legal status of an AI system or component might include: training and inference software licences and terms of use; data licences and terms of use; laws and regulations relating to the location in which training or inference occurs, or where the inferences are used; laws and regulations in the country where the provider of the AI system or component is headquartered; licences and terms of use of the hosting provider; licences and terms of use of a service provider; additional commercial agreements; etc.

The TAIBOM of an AI system is compositional and will inherit the properties of its components. For example, if a system is composed of two third party AI components, one of which was trained on a dataset containing data with a license that prohibited military use, then the system as a whole is likely to be subject to the same prohibition. This compositionality also applies between components such as the hardware, firmware and software of the training and inference environments and any associated licences and legal agreements. 

Rules of composition. For example, software vulnerabilities relating to the training software are not inherited by the inference software

#  Labelling/Versioning

Every aspect of a complex AI system needs labelling and versioning. (data, code and physical systems). Ideally there should be a method of attesting to the version. There can be various trust models to implement this

# Dependencies

A complex AI system has dependencies that need describing to fully understand provenance. TAIBOM will provide an interoperable method of describing these dependencies 

#  Attestation

Any actor (author or third party) can provide descriptors for each component of the system as a whole. (e.g. a training content review, as SBOM validation, a system integrity check, a fairness assessment). 

TAIBOM provides both a mechanism of making these attestations, but also a framework for the dynamic and subjective eva